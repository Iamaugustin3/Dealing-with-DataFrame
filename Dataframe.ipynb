{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/augustine/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/augustine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.json('/home/augustine/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_schema=[StructField('age',IntegerType(),True),\n",
    "             StructField('name',StringType(),True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_struc=StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.json('/home/augustine/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/people.json',schema=final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=None, name='Michael')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['age','name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df.withColumn('newage_double',df['age']*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------------+\n",
      "| age|   name|newage_double|\n",
      "+----+-------+-------------+\n",
      "|null|Michael|         null|\n",
      "|  30|   Andy|           60|\n",
      "|  19| Justin|           38|\n",
      "+----+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|new_age|   name|\n",
      "+-------+-------+\n",
      "|   null|Michael|\n",
      "|     30|   Andy|\n",
      "|     19| Justin|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('age','new_age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('people')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=spark.sql('SELECT * from people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2=SparkSession.builder.appName('ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark2.read.csv('/home/augustine/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/apple_stock_data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+--------+---------+\n",
      "|               Date|  Open|  High|   Low| Close|  Volume|Adj Close|\n",
      "+-------------------+------+------+------+------+--------+---------+\n",
      "|2012-03-30 00:00:00|608.77|610.56|597.94|599.55|26050900|   599.55|\n",
      "|2012-03-29 00:00:00|612.78|616.56|607.23|609.86|21668300|   609.86|\n",
      "|2012-03-28 00:00:00|618.38|621.45|610.31|617.62|23385200|   617.62|\n",
      "|2012-03-27 00:00:00|606.18|616.28|606.06|614.48|21628200|   614.48|\n",
      "|2012-03-26 00:00:00|599.79|607.15|595.26|606.98|21259900|   606.98|\n",
      "|2012-03-23 00:00:00|600.49| 601.8| 594.4|596.05|15359900|   596.05|\n",
      "|2012-03-22 00:00:00|597.78| 604.5|595.53|599.34|22281100|   599.34|\n",
      "|2012-03-21 00:00:00|602.74|609.65|601.41| 602.5|22958200|    602.5|\n",
      "|2012-03-20 00:00:00|599.51| 606.9|591.48|605.96|29166500|   605.96|\n",
      "|2012-03-19 00:00:00|598.37|601.77|589.05| 601.1|32187000|    601.1|\n",
      "|2012-03-16 00:00:00|584.72| 589.2| 578.0|585.57|29481700|   585.57|\n",
      "|2012-03-15 00:00:00|599.61|600.01|578.55|585.56|41418500|   585.56|\n",
      "|2012-03-14 00:00:00|578.05|594.72| 575.4|589.58|50570100|   589.58|\n",
      "|2012-03-13 00:00:00|557.54|568.18|555.75| 568.1|24610500|    568.1|\n",
      "|2012-03-12 00:00:00|548.98| 552.0| 547.0| 552.0|14477800|    552.0|\n",
      "|2012-03-09 00:00:00|544.21|547.74|543.11|545.17|14943500|   545.17|\n",
      "|2012-03-08 00:00:00|534.69|542.99|532.12|541.99|18392600|   541.99|\n",
      "|2012-03-07 00:00:00| 536.8|537.78| 523.3|530.69|28490300|   530.69|\n",
      "|2012-03-06 00:00:00|523.66|533.69|516.22|530.26|28899400|   530.26|\n",
      "|2012-03-05 00:00:00|545.42|547.48| 526.0|533.16|28857200|   533.16|\n",
      "+-------------------+------+------+------+------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  Open| Close|\n",
      "+------+------+\n",
      "|514.26|497.67|\n",
      "|490.96|493.42|\n",
      "|480.76|493.17|\n",
      "| 470.5|476.68|\n",
      "|465.25|468.83|\n",
      "|458.38|463.97|\n",
      "| 457.3|459.68|\n",
      "| 455.9|455.12|\n",
      "|458.41|456.19|\n",
      "|455.59|456.48|\n",
      "|445.71|453.01|\n",
      "|444.34|447.28|\n",
      "|448.36|444.63|\n",
      "|454.44|446.66|\n",
      "| 425.1|420.41|\n",
      "|422.67|427.41|\n",
      "|427.49| 420.3|\n",
      "|430.15|427.75|\n",
      "|426.96|429.11|\n",
      "| 424.2| 424.7|\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df2.filter('Close<500').select('Open','Close').show()\n",
    "df2.filter(df2['Close'] < 500).select('Open','Close').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+--------+---------+\n",
      "|               Date|  Open|  High|   Low| Close|  Volume|Adj Close|\n",
      "+-------------------+------+------+------+------+--------+---------+\n",
      "|2010-01-29 00:00:00|201.08| 202.2|190.25|192.06|44498300|   192.06|\n",
      "|2010-01-28 00:00:00|204.93| 205.5| 198.7|199.29|41910800|   199.29|\n",
      "|2010-01-22 00:00:00|206.78| 207.5|197.16|197.75|31491700|   197.75|\n",
      "|2009-12-01 00:00:00|202.24|202.77|196.83|196.97|16634400|   196.97|\n",
      "|2009-11-30 00:00:00|201.11|201.68|198.77|199.91|15173500|   199.91|\n",
      "|2009-10-27 00:00:00|201.66|202.81|196.45|197.37|27019700|   197.37|\n",
      "|2009-10-20 00:00:00| 200.6|201.75|197.85|198.76|40751400|   198.76|\n",
      "|2007-12-28 00:00:00|200.59|201.56|196.88|199.83|24987400|   199.83|\n",
      "+-------------------+------+------+------+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter((df2['Close'] < 200)&(df2['Open'] > 200)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result2=df2.filter(df2['Low']==196.45).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result2[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark3=SparkSession.builder.appName('aggs').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=spark3.read.csv('/home/augustine/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/sales_info.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|Company|       avg(Sales)|\n",
      "+-------+-----------------+\n",
      "|   APPL|            370.0|\n",
      "|   GOOG|            220.0|\n",
      "|     FB|            610.0|\n",
      "|   MSFT|322.3333333333333|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy('Company').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|min(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     130.0|\n",
      "|   GOOG|     120.0|\n",
      "|     FB|     350.0|\n",
      "|   MSFT|     124.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy('Company').min().show()\n",
    "# df3.groupBy('Company').max().show() - to show max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy('Company').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data=df3.groupBy('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|sum(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|    1480.0|\n",
      "|   GOOG|     660.0|\n",
      "|     FB|    1220.0|\n",
      "|   MSFT|     967.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data.agg({'Sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|Company|max(Sales)|\n",
      "+-------+----------+\n",
      "|   APPL|     750.0|\n",
      "|   GOOG|     340.0|\n",
      "|     FB|     870.0|\n",
      "|   MSFT|     600.0|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_data.agg({'Sales':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions  import countDistinct,avg,stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|Standard_deviation|\n",
      "+------------------+\n",
      "|250.08742410799007|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(stddev('Sales').alias('Standard_deviation')).show()\n",
    "# examlpes\n",
    "# df3.select(avg('Sales').alias('Average_values')).show()\n",
    "# df3.select(countDistinct('Sales').alias('Distinct_values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data=df3.select(stddev('Sales').alias('std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  std1|\n",
      "+------+\n",
      "|250.09|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_data.select(format_number('std',2).alias('std1')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.select(format_number('Sales',2).alias('std')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|     FB|   Carl|870.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.orderBy(df3['Sales'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark4=SparkSession.builder.appName('miss').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4=spark4.read.csv('/home/augustine/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/ContainsNull.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.drop(subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill('No Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| null|\n",
      "|emp2|No Name| null|\n",
      "|emp3|No Name|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill('No Name',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill(0,subset=['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|avg(Sales)|\n",
      "+----------+\n",
      "|     400.5|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_val=df4.select(mean(df4['Sales'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sales=400.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.na.fill(mean_sales,subset=['Sales']).show() #or df4.na.fill(mean_sales,['Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark5=SparkSession.builder.appName('dates').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=spark5.read.csv('/home/augustine/spark-2.4.7-bin-hadoop2.7/python/test_support/sql/apple_stock_data.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+--------+---------+\n",
      "|               Date|  Open|  High|   Low| Close|  Volume|Adj Close|\n",
      "+-------------------+------+------+------+------+--------+---------+\n",
      "|2012-03-30 00:00:00|608.77|610.56|597.94|599.55|26050900|   599.55|\n",
      "|2012-03-29 00:00:00|612.78|616.56|607.23|609.86|21668300|   609.86|\n",
      "|2012-03-28 00:00:00|618.38|621.45|610.31|617.62|23385200|   617.62|\n",
      "|2012-03-27 00:00:00|606.18|616.28|606.06|614.48|21628200|   614.48|\n",
      "|2012-03-26 00:00:00|599.79|607.15|595.26|606.98|21259900|   606.98|\n",
      "|2012-03-23 00:00:00|600.49| 601.8| 594.4|596.05|15359900|   596.05|\n",
      "|2012-03-22 00:00:00|597.78| 604.5|595.53|599.34|22281100|   599.34|\n",
      "|2012-03-21 00:00:00|602.74|609.65|601.41| 602.5|22958200|    602.5|\n",
      "|2012-03-20 00:00:00|599.51| 606.9|591.48|605.96|29166500|   605.96|\n",
      "|2012-03-19 00:00:00|598.37|601.77|589.05| 601.1|32187000|    601.1|\n",
      "|2012-03-16 00:00:00|584.72| 589.2| 578.0|585.57|29481700|   585.57|\n",
      "|2012-03-15 00:00:00|599.61|600.01|578.55|585.56|41418500|   585.56|\n",
      "|2012-03-14 00:00:00|578.05|594.72| 575.4|589.58|50570100|   589.58|\n",
      "|2012-03-13 00:00:00|557.54|568.18|555.75| 568.1|24610500|    568.1|\n",
      "|2012-03-12 00:00:00|548.98| 552.0| 547.0| 552.0|14477800|    552.0|\n",
      "|2012-03-09 00:00:00|544.21|547.74|543.11|545.17|14943500|   545.17|\n",
      "|2012-03-08 00:00:00|534.69|542.99|532.12|541.99|18392600|   541.99|\n",
      "|2012-03-07 00:00:00| 536.8|537.78| 523.3|530.69|28490300|   530.69|\n",
      "|2012-03-06 00:00:00|523.66|533.69|516.22|530.26|28899400|   530.26|\n",
      "|2012-03-05 00:00:00|545.42|547.48| 526.0|533.16|28857200|   533.16|\n",
      "+-------------------+------+------+------+------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2012, 3, 30, 0, 0), Open=608.77, High=610.56, Low=597.94, Close=599.55, Volume=26050900, Adj Close=599.55)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5.select(['Date','Open']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofmonth,dayofyear,month,year,hour,weekofyear,format_number,date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|dayofmonth(Date)|\n",
      "+----------------+\n",
      "|              30|\n",
      "|              29|\n",
      "|              28|\n",
      "|              27|\n",
      "|              26|\n",
      "|              23|\n",
      "|              22|\n",
      "|              21|\n",
      "|              20|\n",
      "|              19|\n",
      "|              16|\n",
      "|              15|\n",
      "|              14|\n",
      "|              13|\n",
      "|              12|\n",
      "|               9|\n",
      "|               8|\n",
      "|               7|\n",
      "|               6|\n",
      "|               5|\n",
      "+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(dayofmonth(df5['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(Date)|\n",
      "+----------+\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "|      2012|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df5.select(hour(df5['Date'])).show()\n",
    "# df5.select(month(df5['Date'])).show()\n",
    "df5.select(year(df5['Date'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf=df5.withColumn('Year',year(df5['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=newdf.groupBy('Year').mean().select(['Year','avg(Close)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+\n",
      "|Year|Average Closing Price|\n",
      "+----+---------------------+\n",
      "|1990|                37.56|\n",
      "|2003|                18.54|\n",
      "|2007|               128.27|\n",
      "|2006|                70.81|\n",
      "|1997|                17.97|\n",
      "|1988|                41.54|\n",
      "|1994|                34.08|\n",
      "|2004|                35.53|\n",
      "|1991|                52.50|\n",
      "|1996|                24.92|\n",
      "|1989|                41.66|\n",
      "|1998|                30.57|\n",
      "|1985|                20.19|\n",
      "|2012|               503.68|\n",
      "|1987|                53.89|\n",
      "|2009|               146.81|\n",
      "|1995|                40.54|\n",
      "|2001|                20.22|\n",
      "|1992|                54.80|\n",
      "|2005|                52.40|\n",
      "+----+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(['Year',format_number('avg(Close)',2).alias('Average Closing Price')]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
